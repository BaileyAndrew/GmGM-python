{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from GmGM import GmGM\n",
    "from GmGM.synthetic import PrecMatGenerator, DatasetGenerator\n",
    "from GmGM.synthetic import measure_prec_recall, plot_prec_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<GmGM.synthetic.generate_data.DatasetGenerator at 0x15d55c520>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = DatasetGenerator(\n",
    "    structure={\n",
    "        \"rna\": (\"cell\", \"gene\"),\n",
    "        \"atac\": (\"cell\", \"peak\"),\n",
    "    },\n",
    "    generator={\n",
    "        \"cell\": PrecMatGenerator(),\n",
    "        \"gene\": PrecMatGenerator(),\n",
    "        \"peak\": PrecMatGenerator(),\n",
    "    },\n",
    "    size={\n",
    "        \"cell\": 51,\n",
    "        \"gene\": 52,\n",
    "        \"peak\": 53\n",
    "    },\n",
    ")\n",
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(\n",
       "\trna: ('', 'cell', 'gene')\n",
       "\tatac: ('', 'cell', 'peak')\n",
       ")\n",
       "Axes(\n",
       "\tcell: 51\n",
       "\t\tPrior: None\n",
       "\t\tGram: Not calculated\n",
       "\t\tEig: Not calculated\n",
       "\tpeak: 53\n",
       "\t\tPrior: None\n",
       "\t\tGram: Not calculated\n",
       "\t\tEig: Not calculated\n",
       "\tgene: 52\n",
       "\t\tPrior: None\n",
       "\t\tGram: Not calculated\n",
       "\t\tEig: Not calculated\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1/5\n",
      "Lambda #0\n",
      "Algorithm: GmGM\n",
      "Attempt 2/5\n",
      "Lambda #0\n",
      "Algorithm: GmGM\n",
      "Attempt 3/5\n",
      "Lambda #0\n",
      "Algorithm: GmGM\n",
      "Attempt 4/5\n",
      "Lambda #0\n",
      "Algorithm: GmGM\n",
      "Attempt 5/5\n",
      "Lambda #0\n",
      "Algorithm: GmGM\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<Figure size 640x480 with 1 Axes>,\n",
       " <Axes: xlabel='Recall', ylabel='Precision'>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: find out why this is getting slower over time - memory leak?\n",
    "results = measure_prec_recall(\n",
    "    generator,\n",
    "    algorithms={\n",
    "        \"GmGM\": lambda dataset, lambdas: GmGM(dataset, lambdas, threshold_method=\"overall\"),\n",
    "    },\n",
    "    Lambdas={\n",
    "        \"GmGM\": np.linspace(0.01, 1, 1)\n",
    "    },\n",
    "    num_attempts=5,\n",
    "    num_samples=10,\n",
    "    verbose=3,\n",
    ")\n",
    "plot_prec_recall(\n",
    "    results,\n",
    "    axis = \"cell\",\n",
    "    generator=generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 2.374 s\n",
      "File: /Users/baileyandrew/mambaforge/envs/GmGM-python-demo/lib/python3.9/site-packages/GmGM/GmGM.py\n",
      "Function: GmGM at line 29\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    29                                           def GmGM(\n",
      "    30                                               dataset: Dataset | AnnData,\n",
      "    31                                               to_keep: float | int | dict[Axis, float | int],\n",
      "    32                                               random_state: Optional[int] = None,\n",
      "    33                                               batch_size: Optional[int] = None,\n",
      "    34                                               verbose: bool = False,\n",
      "    35                                               # `center` parameters\n",
      "    36                                               centering_method: Optional[Literal[\"avg-overall\", \"clr-prost\"]] = None,\n",
      "    37                                               # `create_gram_matrices` parameters\n",
      "    38                                               use_nonparanormal_skeptic: bool = False,\n",
      "    39                                               # `calculate_eigenvectors` parameters\n",
      "    40                                               n_comps: Optional[int] = None,\n",
      "    41                                               # `calculate_eigenvalues` parameters\n",
      "    42                                               max_small_steps: int = 5,\n",
      "    43                                               max_line_search_steps: int = 20,\n",
      "    44                                               lr_init: float = 1.0,\n",
      "    45                                               max_iter: int = 1000,\n",
      "    46                                               tol: float = 1e-3,\n",
      "    47                                               regularizer: Optional[Regularizer] = None,\n",
      "    48                                               force_posdef: bool = True,\n",
      "    49                                               verbose_every: int = 100,\n",
      "    50                                               always_regularize: bool = False,\n",
      "    51                                               check_overstep_each_iter: bool = False,\n",
      "    52                                               # `recompose_sparse_positions` parameters\n",
      "    53                                               threshold_method: Literal[\"overall\", \"rowwise\", \"rowwise-col-weighted\"] = \"rowwise-col-weighted\",\n",
      "    54                                               # from_AnnData/MuData parameters\n",
      "    55                                               use_highly_variable: bool = False,\n",
      "    56                                               key_added: str = \"gmgm\",\n",
      "    57                                               use_abs_of_graph: bool = True,\n",
      "    58                                           ):\n",
      "    59                                               \"\"\"\n",
      "    60                                               Performs GmGM on the given dataset.\n",
      "    61                                               \"\"\"\n",
      "    62                                               # Convert AnnData/MuData to Dataset (if relevant)\n",
      "    63       100      80000.0    800.0      0.0      is_anndata: bool = AnnData is not None and isinstance(dataset, AnnData)\n",
      "    64       100      45000.0    450.0      0.0      is_mudata: bool = MuData is not None and isinstance(dataset, MuData)\n",
      "    65       100      19000.0    190.0      0.0      if is_anndata:\n",
      "    66                                                   _dataset = Dataset.from_AnnData(dataset, use_highly_variable=use_highly_variable)\n",
      "    67       100      19000.0    190.0      0.0      elif is_mudata:\n",
      "    68                                                   _dataset = Dataset.from_MuData(dataset, use_highly_variable=use_highly_variable)\n",
      "    69                                               else:\n",
      "    70       100      21000.0    210.0      0.0          _dataset = dataset\n",
      "    71                                           \n",
      "    72                                               # Save the random state\n",
      "    73       100      36000.0    360.0      0.0      _dataset.random_state = random_state\n",
      "    74                                           \n",
      "    75                                               # Center dataset\n",
      "    76       100      17000.0    170.0      0.0      if verbose:\n",
      "    77                                                   print(\"Centering...\")\n",
      "    78       100      23000.0    230.0      0.0      if centering_method == \"clr-prost\":\n",
      "    79                                                   if is_anndata or is_mudata:\n",
      "    80                                                       if 'log1p' in dataset.uns.keys():\n",
      "    81                                                           warnings.warn(\n",
      "    82                                                               \"Dataset was log1p-transformed; clr-prost expects raw compositional (such as count) data\"\n",
      "    83                                                           )\n",
      "    84                                                   clr_prost(_dataset)\n",
      "    85       100      34000.0    340.0      0.0      elif centering_method == \"avg-overall\":\n",
      "    86                                                   center(_dataset)\n",
      "    87       100      24000.0    240.0      0.0      elif centering_method is None:\n",
      "    88       100      20000.0    200.0      0.0          pass\n",
      "    89                                               else:\n",
      "    90                                                   raise ValueError(f\"Invalid centering method: {centering_method}\")\n",
      "    91                                               \n",
      "    92                                               # Calculate eigenvectors\n",
      "    93       100      30000.0    300.0      0.0      if verbose:\n",
      "    94                                                   print(\"Calculating eigenvectors...\")\n",
      "    95                                               # We can skip the gram matrix calculation by doing SVD directly\n",
      "    96                                               # Bringing memory usage down from O(n^2) to O(n) if `n_comps`` is O(1)\n",
      "    97                                               # In my experience it slows you down if we want all eigenvectors, so\n",
      "    98                                               # we only do this if `n_comps`` is specified\n",
      "    99       100      46000.0    460.0      0.0      unimodal: bool = len(_dataset.dataset) == 1\n",
      "   100       200     277000.0   1385.0      0.0      matrix_variate: bool = all([\n",
      "   101                                                   _dataset.dataset[key].ndim == 2\n",
      "   102       100      57000.0    570.0      0.0          for key in _dataset.dataset.keys()\n",
      "   103                                               ])\n",
      "   104                                               # If dataset is a single matrix, then we can do SVD on said matrix\n",
      "   105                                               # to get the eigenvectors for both axes at once\n",
      "   106       100      24000.0    240.0      0.0      if unimodal and matrix_variate and n_comps is not None:\n",
      "   107                                                   if verbose:\n",
      "   108                                                       print(\"\\tby calculating SVD...\")\n",
      "   109                                                   direct_svd(\n",
      "   110                                                       _dataset,\n",
      "   111                                                       n_comps=n_comps,\n",
      "   112                                                       random_state=random_state,\n",
      "   113                                                   )\n",
      "   114                                               # If dataset is multi-modal or tensor-variate, we can find the left\n",
      "   115                                               # eigenvectors of the concatenation of the matricization of each modality\n",
      "   116                                               # on a given axis\n",
      "   117       100      28000.0    280.0      0.0      elif n_comps is not None:\n",
      "   118                                                   if verbose:\n",
      "   119                                                       print(\"\\tby calculating left eigenvectors of concatenated matricizations...\")\n",
      "   120                                                   direct_left_eigenvectors(\n",
      "   121                                                       _dataset,\n",
      "   122                                                       n_comps=n_comps,\n",
      "   123                                                       random_state=random_state,\n",
      "   124                                                   )\n",
      "   125                                               # If dataset is multi-modal or is tensor-variate, we need to calculate the gram matrices\n",
      "   126                                               # An O(n^2) memory operation\n",
      "   127                                               else:\n",
      "   128       100      32000.0    320.0      0.0          if verbose:\n",
      "   129                                                       print(\"\\tby calculating gram matrices and then eigendecomposing...\")\n",
      "   130                                                   # Create Gram matrices\n",
      "   131       200   55835000.0 279175.0      2.4          create_gram_matrices(\n",
      "   132       100      19000.0    190.0      0.0              _dataset,\n",
      "   133       100      25000.0    250.0      0.0              use_nonparanormal_skeptic=use_nonparanormal_skeptic,\n",
      "   134       100      19000.0    190.0      0.0              batch_size=batch_size\n",
      "   135                                                   )\n",
      "   136                                           \n",
      "   137                                                   # Calculate eigenvectors\n",
      "   138       200 1642969000.0    8e+06     69.2          calculate_eigenvectors(\n",
      "   139       100     238000.0   2380.0      0.0              _dataset,\n",
      "   140       100      30000.0    300.0      0.0              n_comps=n_comps,\n",
      "   141       100      35000.0    350.0      0.0              random_state=random_state,\n",
      "   142       100      25000.0    250.0      0.0              verbose=verbose\n",
      "   143                                                   )\n",
      "   144                                           \n",
      "   145                                               # Calculate eigenvalues\n",
      "   146       100      55000.0    550.0      0.0      if verbose:\n",
      "   147                                                   print(\"Calculating eigenvalues...\")\n",
      "   148       200  348702000.0    2e+06     14.7      calculate_eigenvalues(\n",
      "   149       100    1799000.0  17990.0      0.1          _dataset,\n",
      "   150       100      29000.0    290.0      0.0          max_small_steps=max_small_steps,\n",
      "   151       100      36000.0    360.0      0.0          max_line_search_steps=max_line_search_steps,\n",
      "   152       100      26000.0    260.0      0.0          lr_init=lr_init,\n",
      "   153       100      27000.0    270.0      0.0          max_iter=max_iter,\n",
      "   154       100      38000.0    380.0      0.0          tol=tol,\n",
      "   155       100      28000.0    280.0      0.0          regularizer=regularizer,\n",
      "   156       100      26000.0    260.0      0.0          force_posdef=force_posdef,\n",
      "   157       100      36000.0    360.0      0.0          verbose=verbose,\n",
      "   158       100      24000.0    240.0      0.0          verbose_every=verbose_every,\n",
      "   159       100      36000.0    360.0      0.0          always_regularize=always_regularize,\n",
      "   160       100      32000.0    320.0      0.0          check_overstep_each_iter=check_overstep_each_iter,\n",
      "   161                                               )\n",
      "   162                                           \n",
      "   163                                               # Recompose sparse precisions\n",
      "   164       100      35000.0    350.0      0.0      if verbose:\n",
      "   165                                                   print(\"Recomposing sparse precisions...\")\n",
      "   166       200  322489000.0    2e+06     13.6      recompose_sparse_precisions(\n",
      "   167       100      36000.0    360.0      0.0          _dataset,\n",
      "   168       100      31000.0    310.0      0.0          to_keep=to_keep,\n",
      "   169       100      32000.0    320.0      0.0          threshold_method=threshold_method,\n",
      "   170       100      32000.0    320.0      0.0          batch_size=batch_size\n",
      "   171                                               )\n",
      "   172                                           \n",
      "   173                                               # If was AnnData/MuData, return it\n",
      "   174       100     151000.0   1510.0      0.0      if AnnData is not None and isinstance(dataset, AnnData):\n",
      "   175                                                   if verbose:\n",
      "   176                                                       print(\"Converting back to AnnData...\")\n",
      "   177                                                   return _dataset.to_AnnData(key_added=key_added, use_abs_of_graph=use_abs_of_graph)\n",
      "   178       100      59000.0    590.0      0.0      elif MuData is not None and isinstance(dataset, MuData):\n",
      "   179                                                   if verbose:\n",
      "   180                                                       print(\"Converting back to MuData...\")\n",
      "   181                                                   return _dataset.to_MuData(key_added=key_added, use_abs_of_graph=use_abs_of_graph)\n",
      "   182                                               \n",
      "   183       100     191000.0   1910.0      0.0      if verbose:\n",
      "   184                                                   print(\"Done!\")\n",
      "   185                                               \n",
      "   186                                               # Otherwise, return a Dataset object\n",
      "   187       100      24000.0    240.0      0.0      return _dataset\n",
      "\n",
      "Total time: 0.292329 s\n",
      "File: /Users/baileyandrew/mambaforge/envs/GmGM-python-demo/lib/python3.9/site-packages/GmGM/core/core.py\n",
      "Function: calculate_eigenvalues at line 206\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "   206                                           def calculate_eigenvalues(\n",
      "   207                                               X: Dataset,\n",
      "   208                                               *,\n",
      "   209                                               max_small_steps: int = 5,\n",
      "   210                                               max_line_search_steps: int = 20,\n",
      "   211                                               lr_init: float = 1.0,\n",
      "   212                                               max_iter: int = 1000,\n",
      "   213                                               tol: float = 1e-3,\n",
      "   214                                               regularizer: Optional[Regularizer] = None,\n",
      "   215                                               force_posdef: bool = True,\n",
      "   216                                               verbose: bool = False,\n",
      "   217                                               verbose_every: int = 100,\n",
      "   218                                               always_regularize: bool = False,\n",
      "   219                                               check_overstep_each_iter: bool = False,\n",
      "   220                                           ) -> Dataset:\n",
      "   221                                           \n",
      "   222                                               # Initialize looping variables\n",
      "   223       100      28000.0    280.0      0.0      num_small_steps: int = 0\n",
      "   224       100      23000.0    230.0      0.0      lr_t: float = lr_init\n",
      "   225       100      56000.0    560.0      0.0      prev_err: float = np.inf\n",
      "   226       100      20000.0    200.0      0.0      regularizing: bool = always_regularize and not regularizer is None\n",
      "   227                                           \n",
      "   228                                               # Eigenvalues of the mle, keyed by axis\n",
      "   229       200    1635000.0   8175.0      0.6      X.evals: dict[Axis, np.ndarray] = {\n",
      "   230                                                   axis: np.ones_like(e)\n",
      "   231       100      37000.0    370.0      0.0          for axis, e in X.es.items()\n",
      "   232                                               }\n",
      "   233                                           \n",
      "   234                                               # Changes in eigenvalues in current iteration\n",
      "   235                                               # keyed by axis\n",
      "   236       200     905000.0   4525.0      0.3      diffs: dict[Axis, np.ndarray] = {\n",
      "   237                                                   axis: np.zeros_like(e)\n",
      "   238       100      37000.0    370.0      0.0          for axis, e in X.es.items()\n",
      "   239                                               }\n",
      "   240                                           \n",
      "   241                                               # Converge to eigenvalue MLE\n",
      "   242      1320     459000.0    347.7      0.2      for i in range(max_iter):\n",
      "   243                                           \n",
      "   244                                                   # Compute MLE gradient\n",
      "   245      5280    1813000.0    343.4      0.6          for axis, locations in X.presences_batchless.items():\n",
      "   246      3960    1030000.0    260.1      0.4              assert axis not in X.batch_axes, \\\n",
      "   247                                                           \"Batch axes should not be in presences_batchless\"\n",
      "   248      3960    5341000.0   1348.7      1.8              diffs[axis] = X.es[axis].copy()\n",
      "   249     11880    4761000.0    400.8      1.6              for modality, ell in locations.items():\n",
      "   250      7920    1801000.0    227.4      0.6                  if ell is None:\n",
      "   251      2640     455000.0    172.3      0.2                      continue\n",
      "   252                                                           # Note to self: often it seems project_inv_kron_sum\n",
      "   253                                                           # is irrelevant, because it doesn't affect much...\n",
      "   254                                                           # Presumably the log determinant is much smaller in effect\n",
      "   255                                                           # than the trace...\n",
      "   256                                                           # Could be worth investigating how often this is the case\n",
      "   257                                                           # and what this says about multi-axis methods\n",
      "   258     15840   90202000.0   5694.6     30.9                  diffs[axis] -= project_inv_kron_sum(\n",
      "   259      5280    1369000.0    259.3      0.5                      X.evals,\n",
      "   260      5280    1228000.0    232.6      0.4                      X.structure,\n",
      "   261      5280    1204000.0    228.0      0.4                      modality,\n",
      "   262      5280    1137000.0    215.3      0.4                      X.batch_axes,\n",
      "   263      5280    1451000.0    274.8      0.5                      X.Ks[modality],\n",
      "   264      5280    1280000.0    242.4      0.4                  )[ell]\n",
      "   265                                           \n",
      "   266      5280    2841000.0    538.1      1.0                  for axis, prior in X.prior.items():\n",
      "   267                                                               diffs[axis] += prior.process_gradient(\n",
      "   268                                                                   X.evals[axis]\n",
      "   269                                                               )\n",
      "   270                                           \n",
      "   271                                                   # Add regularization if necessary\n",
      "   272                                                   # Only activates after converging to the MLE\n",
      "   273      1320     360000.0    272.7      0.1          if regularizing:\n",
      "   274                                                       regs: dict[str, np.ndarray] = regularizer.grad(\n",
      "   275                                                           X.evals,\n",
      "   276                                                           X.evecs,\n",
      "   277                                                       )\n",
      "   278                                                       for axis in X.all_axes:\n",
      "   279                                                           diffs[axis] += regs[axis]\n",
      "   280                                           \n",
      "   281                                                   # Backtracking line search\n",
      "   282      1320     340000.0    257.6      0.1          line_search_gave_up: bool = False\n",
      "   283      1320     356000.0    269.7      0.1          lr_t: float = lr_init\n",
      "   284      3360    1724000.0    513.1      0.6          for line_step in range(max_line_search_steps):\n",
      "   285                                                       # Decrease step size each time\n",
      "   286                                                       # (`line_step` starts at 0, i.e. no decrease)\n",
      "   287      3360    1976000.0    588.1      0.7              step_lr: float = lr_t / 10**line_step\n",
      "   288                                                       \n",
      "   289     13440    4453000.0    331.3      1.5              for axis in X.all_axes:\n",
      "   290     10080   22552000.0   2237.3      7.7                  X.evals[axis] -= step_lr * diffs[axis]\n",
      "   291                                           \n",
      "   292                                                       # Since all tuplets of eigenvalues\n",
      "   293                                                       # get summed together within each dataset,\n",
      "   294                                                       # the minimum final eigenvalue is \n",
      "   295                                                       # the sum of minimum axis eigenvalues\n",
      "   296      3360     995000.0    296.1      0.3              if not force_posdef:\n",
      "   297                                                           minimum_diag: float = min(\n",
      "   298                                                               sum(\n",
      "   299                                                                   X.evals[axis].min()\n",
      "   300                                                                   for axis in axes\n",
      "   301                                                                   if axis not in X.batch_axes\n",
      "   302                                                               )\n",
      "   303                                                               for axes in X.structure.values()\n",
      "   304                                                           )\n",
      "   305                                                       # Or we can enforce individual posdefness!\n",
      "   306                                                       else:\n",
      "   307      6720   41376000.0   6157.1     14.2                  minimum_diag = min(\n",
      "   308                                                           min(\n",
      "   309                                                               X.evals[axis].min()\n",
      "   310                                                               for axis in axes\n",
      "   311                                                               if axis not in X.batch_axes\n",
      "   312                                                           )\n",
      "   313      3360    1456000.0    433.3      0.5                  for axes in X.structure.values()\n",
      "   314                                                           )\n",
      "   315                                           \n",
      "   316                                                       # If the minimum eigenvalue is less than zero\n",
      "   317                                                       # have we left the positive definite space we desire\n",
      "   318                                                       # to stay in, so we will have to backtrack\n",
      "   319      3360    1273000.0    378.9      0.4              if minimum_diag <= 1e-8:\n",
      "   320      8160    2856000.0    350.0      1.0                  for axis in X.all_axes:\n",
      "   321      6120   19506000.0   3187.3      6.7                      X.evals[axis] += step_lr * diffs[axis]\n",
      "   322      2040     485000.0    237.7      0.2                  continue\n",
      "   323                                                       \n",
      "   324                                           \n",
      "   325                                                       # Check if error has gotten worse\n",
      "   326      1320     456000.0    345.5      0.2              if check_overstep_each_iter:\n",
      "   327                                                           log_err: float = _get_log_err(\n",
      "   328                                                               X.evals,\n",
      "   329                                                               X.structure\n",
      "   330                                                           )\n",
      "   331                                                           trace_err: float = _get_trace_err(\n",
      "   332                                                               X.es,\n",
      "   333                                                               X.evals,\n",
      "   334                                                               X.all_axes\n",
      "   335                                                           )\n",
      "   336                                                           reg_err: float = _get_reg_err(\n",
      "   337                                                               X.evals,\n",
      "   338                                                               X.evecs,\n",
      "   339                                                               regularizing,\n",
      "   340                                                               regularizer\n",
      "   341                                                           )\n",
      "   342                                                           err: float = log_err + trace_err + reg_err\n",
      "   343                                                           if err > prev_err:\n",
      "   344                                                               for axis in X.all_axes:\n",
      "   345                                                                   X.evals[axis] += step_lr * diffs[axis]\n",
      "   346                                                               continue\n",
      "   347                                                       \n",
      "   348                                                       # If it got here, we have a good step size\n",
      "   349      1320     461000.0    349.2      0.2              break\n",
      "   350                                                   else:\n",
      "   351                                                       # Did not find a good step size\n",
      "   352                                                       if verbose:\n",
      "   353                                                           print(f\"@{i}: {prev_err} - Line Search Gave Up!\")\n",
      "   354                                                       line_search_gave_up = True\n",
      "   355                                                       num_small_steps = max_line_search_steps + 1\n",
      "   356                                           \n",
      "   357                                                   # Calculate the error\n",
      "   358      1320     484000.0    366.7      0.2          if not check_overstep_each_iter:\n",
      "   359      2640   50159000.0  18999.6     17.2              log_err: float = _get_log_err(\n",
      "   360      1320     532000.0    403.0      0.2                  X.evals,\n",
      "   361      1320     502000.0    380.3      0.2                  X.structure\n",
      "   362                                                       )\n",
      "   363      2640    9123000.0   3455.7      3.1              trace_err: float = _get_trace_err(\n",
      "   364      1320     523000.0    396.2      0.2                  X.es,\n",
      "   365      1320     475000.0    359.8      0.2                  X.evals,\n",
      "   366      1320     494000.0    374.2      0.2                  X.all_axes\n",
      "   367                                                       )\n",
      "   368      2640    1589000.0    601.9      0.5              reg_err: float = _get_reg_err(\n",
      "   369      1320     707000.0    535.6      0.2                  X.evals,\n",
      "   370      1320     532000.0    403.0      0.2                  X.evecs,\n",
      "   371      1320     504000.0    381.8      0.2                  regularizing,\n",
      "   372      1320     513000.0    388.6      0.2                  regularizer\n",
      "   373                                                       )\n",
      "   374      1320     769000.0    582.6      0.3              err: float = log_err + trace_err + reg_err\n",
      "   375                                           \n",
      "   376                                                   # Calculate the change in error and\n",
      "   377                                                   # whether or not we can consider\n",
      "   378                                                   # ourselves to be converged\n",
      "   379      1320    2068000.0   1566.7      0.7          err_diff: float = np.abs(prev_err - err)\n",
      "   380      1320     505000.0    382.6      0.2          prev_err: float = err\n",
      "   381      1320    1797000.0   1361.4      0.6          if err_diff/np.abs(err) < tol or line_search_gave_up:\n",
      "   382       500     256000.0    512.0      0.1              num_small_steps += 1\n",
      "   383       500     241000.0    482.0      0.1              if num_small_steps >= max_small_steps:\n",
      "   384       100      40000.0    400.0      0.0                  if verbose:\n",
      "   385                                                               print(f\"Converged! (@{i}: {err})\")\n",
      "   386       100      39000.0    390.0      0.0                  if not regularizing and regularizer is not None:\n",
      "   387                                                               if verbose:\n",
      "   388                                                                   print(\"Regularizing!\")\n",
      "   389                                                               tol /= 10\n",
      "   390                                                               regularizing = True\n",
      "   391                                                               num_small_steps = 0\n",
      "   392                                                           else:\n",
      "   393       100      61000.0    610.0      0.0                      break\n",
      "   394                                                   else:\n",
      "   395       820     356000.0    434.1      0.1              num_small_steps = 0\n",
      "   396                                           \n",
      "   397      1220     303000.0    248.4      0.1          if verbose:\n",
      "   398                                                       if i % verbose_every == 0:\n",
      "   399                                                           print(f\"@{i}: {err} ({log_err} + {trace_err} + {reg_err}) ∆{err_diff / np.abs(err)}\")\n",
      "   400                                               else:\n",
      "   401                                                   # This triggers if we don't break out of the loop\n",
      "   402                                                   if verbose:\n",
      "   403                                                       print(\"Did not converge!\")\n",
      "   404                                           \n",
      "   405       100      19000.0    190.0      0.0      return X\n",
      "\n",
      "Total time: 2.63888 s\n",
      "File: /Users/baileyandrew/mambaforge/envs/GmGM-python-demo/lib/python3.9/site-packages/GmGM/synthetic/validation.py\n",
      "Function: measure_prec_recall at line 35\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    35                                           def measure_prec_recall(\n",
      "    36                                               generator: DatasetGenerator,\n",
      "    37                                               algorithms: dict[AlgorithmName, Algorithm],\n",
      "    38                                               Lambdas: dict[AlgorithmName, list[float]],\n",
      "    39                                               num_attempts: int,\n",
      "    40                                               num_samples: int,\n",
      "    41                                               *,\n",
      "    42                                               verbose: int = 0,\n",
      "    43                                               give_prior: bool = False\n",
      "    44                                           ) -> dict[\n",
      "    45                                               AlgorithmName,\n",
      "    46                                               dict[\n",
      "    47                                                   AxisName,\n",
      "    48                                                   dict[\n",
      "    49                                                       MetricName,\n",
      "    50                                                       list[float]\n",
      "    51                                                   ]\n",
      "    52                                               ]\n",
      "    53                                           ]:\n",
      "    54                                               \"\"\"\n",
      "    55                                               Using `generator`, test the performance of `algorithm` on the dataset\n",
      "    56                                               as the regularization parameter Lambda varies\n",
      "    57                                           \n",
      "    58                                               For each Lambda, we run `num_attempts` attempts and average the results\n",
      "    59                                               \"\"\"\n",
      "    60                                           \n",
      "    61         1       2000.0   2000.0      0.0      random_alg = list(algorithms.keys())[0]\n",
      "    62         1       1000.0   1000.0      0.0      output = [None] * len(Lambdas[random_alg])\n",
      "    63                                           \n",
      "    64         1       1000.0   1000.0      0.0      num_Lambdas = len(Lambdas[random_alg])\n",
      "    65                                           \n",
      "    66                                               # Create measurers\n",
      "    67         1          0.0      0.0      0.0      measurers = []\n",
      "    68        21       4000.0    190.5      0.0      for idx in range(num_Lambdas):\n",
      "    69        40     193000.0   4825.0      0.0          measurers.append({\n",
      "    70                                                       algorithm_name: {\n",
      "    71                                                           axis_name: {\n",
      "    72                                                               metric_name: RunningMeasurer()\n",
      "    73                                                               for metric_name in [\"precision\", \"recall\"]\n",
      "    74                                                           }\n",
      "    75                                                           for axis_name in generator.axes\n",
      "    76                                                       }\n",
      "    77        20       1000.0     50.0      0.0              for algorithm_name in algorithms.keys()\n",
      "    78                                                   })\n",
      "    79                                           \n",
      "    80         6       2000.0    333.3      0.0      for i in range(num_attempts):\n",
      "    81         5          0.0      0.0      0.0          if verbose >= 1:\n",
      "    82                                                       print(f\"Attempt {i+1}/{num_attempts}\")\n",
      "    83                                           \n",
      "    84                                                   # Generate a new ground truth\n",
      "    85         5  129117000.0    3e+07      4.9          generator.reroll_Psis()\n",
      "    86         5      11000.0   2200.0      0.0          Psis_true = generator.Psis\n",
      "    87                                           \n",
      "    88                                                   # Use this new ground truth to generate\n",
      "    89                                                   # an input dataset\n",
      "    90         5  111993000.0    2e+07      4.2          dataset = generator.generate(num_samples)\n",
      "    91                                           \n",
      "    92       105      39000.0    371.4      0.0          for idx in range(num_Lambdas):\n",
      "    93       100      32000.0    320.0      0.0              if verbose >= 2:\n",
      "    94                                                           print(f\"Lambda #{idx}\")\n",
      "    95                                           \n",
      "    96                                                       # For each algorithm collect metrics for that\n",
      "    97                                                       # algorithm on this dataset\n",
      "    98       200     122000.0    610.0      0.0              for algorithm_name, algorithm in algorithms.items():\n",
      "    99       100      27000.0    270.0      0.0                  if verbose >= 3:\n",
      "   100                                                               print(f\"Algorithm: {algorithm_name}\")\n",
      "   101                                           \n",
      "   102                                                           # Run algorithm\n",
      "   103       100      16000.0    160.0      0.0                  if not give_prior:\n",
      "   104       200 2376436000.0    1e+07     90.1                      Psis_pred = algorithm(\n",
      "   105       100      13000.0    130.0      0.0                          dataset,\n",
      "   106       100      69000.0    690.0      0.0                          Lambdas[algorithm_name][idx],\n",
      "   107                                                               )\n",
      "   108                                                           else:\n",
      "   109                                                               Psis_pred = algorithm(\n",
      "   110                                                                   dataset,\n",
      "   111                                                                   Lambdas[algorithm_name][idx],\n",
      "   112                                                                   Psis_true,\n",
      "   113                                                               )\n",
      "   114                                           \n",
      "   115                                                           # Get metrics\n",
      "   116       100   18259000.0 182590.0      0.7                  Psis_pred = binarize_precmats(Psis_pred, eps=1e-3, mode=\"<Tolerance\")\n",
      "   117       200     234000.0   1170.0      0.0                  cm = {\n",
      "   118                                                               axis: generate_confusion_matrices(Psis_pred[axis], Psis_true[axis])\n",
      "   119       100      82000.0    820.0      0.0                      for axis in generator.axes\n",
      "   120                                                               if axis in Psis_pred\n",
      "   121                                                           }\n",
      "   122                                           \n",
      "   123       200     253000.0   1265.0      0.0                  precisions = {\n",
      "   124                                                               axis: precision(cm[axis])\n",
      "   125                                                               if axis in cm\n",
      "   126                                                               else 0\n",
      "   127       100      33000.0    330.0      0.0                      for axis in generator.axes\n",
      "   128                                                           }\n",
      "   129       200     202000.0   1010.0      0.0                  recalls = {\n",
      "   130                                                               axis: recall(cm[axis])\n",
      "   131                                                               if axis in cm\n",
      "   132                                                               else 0\n",
      "   133       100      22000.0    220.0      0.0                      for axis in generator.axes\n",
      "   134                                                           }\n",
      "   135                                           \n",
      "   136                                                           # Keep count in a Running Measurer\n",
      "   137       400     156000.0    390.0      0.0                  for axis in generator.axes:\n",
      "   138       300     800000.0   2666.7      0.0                      measurers[idx][algorithm_name][axis][\"precision\"](precisions[axis])\n",
      "   139       300     447000.0   1490.0      0.0                      measurers[idx][algorithm_name][axis][\"recall\"](recalls[axis])\n",
      "   140                                           \n",
      "   141        21       4000.0    190.5      0.0      for idx in range(num_Lambdas):\n",
      "   142                                                   # Get results from the running measurer into a nice dictionary format\n",
      "   143        40      74000.0   1850.0      0.0          output[idx] = {\n",
      "   144                                                       algorithm_name: {\n",
      "   145                                                           axis_name: {\n",
      "   146                                                               metric_name: measurers[idx][algorithm_name][axis_name][metric_name].mean\n",
      "   147                                                               for metric_name in [\"precision\", \"recall\"]\n",
      "   148                                                           }\n",
      "   149                                                           for axis_name in generator.axes\n",
      "   150                                                       }\n",
      "   151        20       4000.0    200.0      0.0              for algorithm_name in algorithms.keys()\n",
      "   152                                                   }\n",
      "   153                                           \n",
      "   154                                                   # Add the standated deviations into precision_std and recall_std keys\n",
      "   155        40      13000.0    325.0      0.0          for algorithm_name in algorithms.keys():\n",
      "   156        80      22000.0    275.0      0.0              for axis_name in generator.axes:\n",
      "   157       180      47000.0    261.1      0.0                  for metric_name in [\"precision\", \"recall\"]:\n",
      "   158       120      47000.0    391.7      0.0                      output[idx][algorithm_name][axis_name][metric_name + \"_std\"] = \\\n",
      "   159       120     100000.0    833.3      0.0                          measurers[idx][algorithm_name][axis_name][metric_name].std()\n",
      "   160                                           \n",
      "   161         1          0.0      0.0      0.0      return output"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    results = measure_prec_recall(\n",
    "        generator,\n",
    "        algorithms={\n",
    "            \"GmGM\": lambda dataset, lambdas: GmGM(dataset, lambdas, threshold_method=\"overall\"),\n",
    "        },\n",
    "        Lambdas={\n",
    "            \"GmGM\": np.linspace(0.01, 1, 20)\n",
    "        },\n",
    "        num_attempts=5,\n",
    "        num_samples=10,\n",
    "        verbose=0,\n",
    "    )\n",
    "from GmGM import calculate_eigenvalues\n",
    "#print(_project_inv_kron_sum.two_axis.inspect_types())\n",
    "%load_ext line_profiler\n",
    "%lprun -f measure_prec_recall -f GmGM -f calculate_eigenvalues test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'GmGM': {'peak': {'precision': 0.0,\n",
       "    'recall': 0.0,\n",
       "    'precision_std': 0.0,\n",
       "    'recall_std': 0.0},\n",
       "   'gene': {'precision': 0.0,\n",
       "    'recall': 0.0,\n",
       "    'precision_std': 0.0,\n",
       "    'recall_std': 0.0},\n",
       "   'cell': {'precision': 0.0,\n",
       "    'recall': 0.0,\n",
       "    'precision_std': 0.0,\n",
       "    'recall_std': 0.0}}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "def two_axis(\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray\n",
    ") -> tuple[np.ndarray]:\n",
    "    k_ratio: float = 1./2.\n",
    "    x_out: np.ndarray = np.zeros(x.shape[0])\n",
    "    y_out: np.ndarray = np.zeros(y.shape[0])\n",
    "\n",
    "    for i in nb.prange(x.shape[0]):\n",
    "        for j in nb.prange(y.shape[0]):\n",
    "            cur_val: float = 1 / (x[i]+y[j])\n",
    "            x_out[i] += cur_val\n",
    "            y_out[j] += cur_val\n",
    "\n",
    "    # Normalize\n",
    "    x_out /= x.shape[0]\n",
    "    y_out /= y.shape[0]\n",
    "\n",
    "    # Offset diagonal\n",
    "    x_out -= k_ratio * np.sum(x_out) / x.shape[0]\n",
    "    y_out -= k_ratio * np.sum(y_out) / y.shape[0]\n",
    "\n",
    "    return x_out, y_out\n",
    "\n",
    "two_axis_numba = nb.jit(\n",
    "    nb.types.UniTuple(nb.float64[:],2)(nb.float64[:],nb.float64[:]),\n",
    "    nopython=True,\n",
    "    fastmath=True,\n",
    "    parallel=False,\n",
    "    cache=True\n",
    ")(two_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warmup over\n",
      "920 µs ± 30.2 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "9.61 µs ± 55.6 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1_000):\n",
    "    x = 1+np.random.rand(50)\n",
    "    y = 1+np.random.rand(50)\n",
    "    warmup = two_axis_numba(x,y)\n",
    "print(\"warmup over\")\n",
    "%timeit two_axis(x,y)\n",
    "%timeit two_axis_numba(x,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GmGM-python-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
